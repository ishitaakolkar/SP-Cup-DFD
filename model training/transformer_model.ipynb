{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUP5QtCABvVx",
        "outputId": "1cfcce5a-f10f-4589-ce3c-a0f484ef009a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3s4WgB9odpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518caba6-dabf-421c-eadb-badbc4947cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured GPU with a memory limit of 11000 MB.\n",
            "Loading preprocessed features and labels...\n",
            "Epoch 1/5\n",
            "\u001b[1m2743/2743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 225ms/step - accuracy: 0.9820 - auc: 0.9977 - loss: 0.0982 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0037 - learning_rate: 4.9790e-06\n",
            "Epoch 2/5\n",
            "\u001b[1m2743/2743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 206ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 1.5992e-04 - learning_rate: 3.2061e-05\n",
            "Epoch 3/5\n",
            "\u001b[1m2743/2743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 219ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.2859e-04 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 1.4247e-05 - learning_rate: 9.4346e-05\n",
            "Epoch 4/5\n",
            "\u001b[1m2743/2743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 219ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 7.6810e-06 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 2.7384e-06 - learning_rate: 5.1044e-05\n",
            "Epoch 5/5\n",
            "\u001b[1m2743/2743\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 205ms/step - accuracy: 1.0000 - auc: 1.0000 - loss: 1.9956e-06 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 6.2579e-07 - learning_rate: 7.1697e-06\n",
            "Restoring model weights from the end of the best epoch: 5.\n",
            "Evaluating ensemble model...\n",
            "\u001b[1m6858/6858\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 34ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00    109714\n",
            "         1.0       1.00      1.00      1.00    109714\n",
            "\n",
            "    accuracy                           1.00    219428\n",
            "   macro avg       1.00      1.00      1.00    219428\n",
            "weighted avg       1.00      1.00      1.00    219428\n",
            "\n",
            "AUC-ROC: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "import multiprocessing\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras.layers import (Dense, Dropout, BatchNormalization, Conv1D,\n",
        "                                     GlobalAveragePooling1D, Input, concatenate,\n",
        "                                     Flatten, MultiHeadAttention)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "\n",
        "# GPU configuration\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    for device in physical_devices:\n",
        "        try:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "            tf.config.set_logical_device_configuration(\n",
        "                device, [tf.config.LogicalDeviceConfiguration(memory_limit=11000)]\n",
        "            )\n",
        "            print(f\"Configured GPU with a memory limit of 11000 MB.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 25\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# File paths\n",
        "MODEL_SAVE_PATH = \"drive/MyDrive/SP_cup/main_sp.keras\"\n",
        "FEATURES_PATH = \"drive/MyDrive/SP_cup/features_sp.npy\"\n",
        "LABELS_PATH = \"drive/MyDrive/SP_cup/labels_sp.npy\"\n",
        "FAKE_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n",
        "REAL_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n",
        "\n",
        "# Load features from pickle files\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Validate and extract features with multithreading\n",
        "def validate_and_extract(features):\n",
        "    def extract_feature(entry):\n",
        "        if isinstance(entry, list):\n",
        "            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            return [entry['features']]\n",
        "        return []\n",
        "\n",
        "    valid_features = []\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(extract_feature, features), desc=\"Validating and extracting features\"))\n",
        "        for res in results:\n",
        "            valid_features.extend(res)\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Prepare data with parallel processing\n",
        "def prepare_data(fake_path, real_path):\n",
        "    with multiprocessing.Pool(processes=2) as pool:\n",
        "        fake_features, real_features = pool.map(validate_and_extract,\n",
        "                                                [load_features(fake_path), load_features(real_path)])\n",
        "    scaler = StandardScaler()\n",
        "    all_features = np.vstack((fake_features, real_features))\n",
        "    scaler.fit(all_features)\n",
        "\n",
        "    fake_features = scaler.transform(fake_features)\n",
        "    real_features = scaler.transform(real_features)\n",
        "\n",
        "    fake_labels = np.zeros(len(fake_features))\n",
        "    real_labels = np.ones(len(real_features))\n",
        "\n",
        "    X = np.vstack((fake_features, real_features))\n",
        "    y = np.hstack((fake_labels, real_labels))\n",
        "    return X, y\n",
        "\n",
        "# Oversample the minority class\n",
        "def balance_data(X, y):\n",
        "    fake_indices = np.where(y == 0)[0]\n",
        "    real_indices = np.where(y == 1)[0]\n",
        "\n",
        "    real_upsampled = resample(real_indices, replace=True, n_samples=len(fake_indices), random_state=SEED)\n",
        "    balanced_indices = np.concatenate([fake_indices, real_upsampled])\n",
        "    np.random.shuffle(balanced_indices)\n",
        "\n",
        "    return X[balanced_indices], y[balanced_indices]\n",
        "\n",
        "# Feature augmentation with multiprocessing\n",
        "def add_noise(chunk):\n",
        "    \"\"\"Add noise to a data chunk.\"\"\"\n",
        "    noise = np.random.normal(0, 0.01, chunk.shape)\n",
        "    return chunk + noise\n",
        "\n",
        "def augment_data(X):\n",
        "    \"\"\"Apply noise augmentation using multiprocessing.\"\"\"\n",
        "    chunk_size = len(X) // multiprocessing.cpu_count()\n",
        "    chunks = [X[i:i + chunk_size] for i in range(0, len(X), chunk_size)]\n",
        "\n",
        "    with multiprocessing.Pool() as pool:\n",
        "        augmented_chunks = pool.map(add_noise, chunks)\n",
        "\n",
        "    return np.vstack(augmented_chunks)\n",
        "\n",
        "\n",
        "# Build CNN model\n",
        "def build_cnn_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv1D(filters=64, kernel_size=3, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# Build Transformer model\n",
        "def build_transformer_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Flatten()(x)\n",
        "    return Model(inputs, x)\n",
        "\n",
        "# Main pipeline\n",
        "if os.path.exists(FEATURES_PATH) and os.path.exists(LABELS_PATH):\n",
        "    print(\"Loading preprocessed features and labels...\")\n",
        "    X = np.load(FEATURES_PATH)\n",
        "    y = np.load(LABELS_PATH)\n",
        "else:\n",
        "    print(\"Loading and preprocessing raw features...\")\n",
        "    X, y = prepare_data(FAKE_FEATURES_PATH, REAL_FEATURES_PATH)\n",
        "    np.save(FEATURES_PATH, X)\n",
        "    np.save(LABELS_PATH, y)\n",
        "\n",
        "# Parallelized data augmentation and balancing\n",
        "X = augment_data(X)\n",
        "X_balanced, y_balanced = balance_data(X, y)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_balanced), y=y_balanced)\n",
        "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "# Prepare input shape\n",
        "input_shape = (X_balanced.shape[1], 1)\n",
        "\n",
        "# Build models\n",
        "cnn_model = build_cnn_model(input_shape)\n",
        "transformer_model = build_transformer_model(input_shape)\n",
        "\n",
        "# Combine outputs\n",
        "combined_output = concatenate([cnn_model.output, transformer_model.output])\n",
        "final_output = Dense(1, activation='sigmoid')(combined_output)\n",
        "ensemble_model = Model(inputs=[cnn_model.input, transformer_model.input], outputs=final_output)\n",
        "\n",
        "# Compile model\n",
        "lr_schedule = CosineDecayRestarts(initial_learning_rate=1e-4, first_decay_steps=1000, t_mul=2.0, alpha=0.01)\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "ensemble_model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    ModelCheckpoint(MODEL_SAVE_PATH, save_best_only=True, monitor='val_loss', mode='min'),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = ensemble_model.fit(\n",
        "    [X_balanced[..., np.newaxis], X_balanced[..., np.newaxis]], y_balanced,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating ensemble model...\")\n",
        "y_pred = ensemble_model.predict([X_balanced[..., np.newaxis], X_balanced[..., np.newaxis]])\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_balanced, y_pred_binary))\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_balanced, y_pred):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import mixed_precision\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import os\n",
        "import concurrent.futures\n",
        "import multiprocessing\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras.layers import (Dense, Dropout, BatchNormalization, Conv1D,\n",
        "                                     GlobalAveragePooling1D, Input, concatenate,\n",
        "                                     Flatten, MultiHeadAttention)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "# Mixed precision setup\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "# File paths for validation features\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "\n",
        "# Load features from pickle files\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Validate and extract features with parallelism\n",
        "def validate_and_extract(features):\n",
        "    def extract_feature(entry):\n",
        "        if isinstance(entry, list):\n",
        "            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            return [entry['features']]\n",
        "        return []\n",
        "\n",
        "    valid_features = []\n",
        "    with joblib.Parallel(n_jobs=-1, backend='threading') as parallel:\n",
        "        results = parallel(joblib.delayed(extract_feature)(entry) for entry in features)\n",
        "        valid_features.extend([item for sublist in results for item in sublist])\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Prepare data with parallel processing using joblib\n",
        "def prepare_data(fake_path, real_path):\n",
        "    fake_features = load_features(fake_path)\n",
        "    real_features = load_features(real_path)\n",
        "\n",
        "    # Parallel extraction using joblib\n",
        "    fake_features = validate_and_extract(fake_features)\n",
        "    real_features = validate_and_extract(real_features)\n",
        "\n",
        "    # Scaling features\n",
        "    scaler = StandardScaler()\n",
        "    all_features = np.vstack((fake_features, real_features))\n",
        "    scaler.fit(all_features)\n",
        "    fake_features = scaler.transform(fake_features)\n",
        "    real_features = scaler.transform(real_features)\n",
        "\n",
        "    fake_labels = np.zeros(len(fake_features))\n",
        "    real_labels = np.ones(len(real_features))\n",
        "\n",
        "    X = np.vstack((fake_features, real_features))\n",
        "    y = np.hstack((fake_labels, real_labels))\n",
        "    return X, y\n",
        "\n",
        "# Load and prepare the validation data\n",
        "X_valid, y_valid = prepare_data(FAKE_VALID_FEATURES_PATH, REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "# Standardize the validation data\n",
        "scaler = StandardScaler()\n",
        "X_valid = scaler.fit_transform(X_valid)\n",
        "\n",
        "# Reshape the data for model input\n",
        "X_valid = X_valid[..., np.newaxis]  # Ensure the shape is (samples, features, 1)\n",
        "\n",
        "# Load the trained model without loading optimizer weights for efficiency\n",
        "ensemble_model = load_model('drive/MyDrive/SP_cup/main_sp.keras', compile=False)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Evaluating ensemble model on validation data...\")\n",
        "y_pred = ensemble_model.predict([X_valid, X_valid], batch_size=64)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Output the classification report and AUC-ROC score\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_valid, y_pred_binary))\n",
        "print(f\"AUC-ROC: {roc_auc_score(y_valid, y_pred):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbwbA_NDfYoH",
        "outputId": "a0943725-ca81-4e1c-952c-bc51fe49edf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating ensemble model on validation data...\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 61ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      1.00      0.66      1524\n",
            "         1.0       0.00      0.00      0.00      1548\n",
            "\n",
            "    accuracy                           0.50      3072\n",
            "   macro avg       0.25      0.50      0.33      3072\n",
            "weighted avg       0.25      0.50      0.33      3072\n",
            "\n",
            "AUC-ROC: 0.4291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Enable mixed precision\n",
        "set_global_policy(Policy('mixed_float16'))\n",
        "\n",
        "# GPU Configuration: Set memory growth and limit\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"GPU memory growth enabled.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# File paths\n",
        "FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n",
        "REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/model_optimized.keras\"\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Parallel feature validation and extraction\n",
        "def validate_and_extract(features):\n",
        "    def extract_feature(entry):\n",
        "        if isinstance(entry, list):\n",
        "            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            return [entry['features']]\n",
        "        return []\n",
        "\n",
        "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
        "        results = parallel(delayed(extract_feature)(entry) for entry in features)\n",
        "    valid_features = [item for sublist in results for item in sublist]\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Augment features for robustness\n",
        "def augment_features(X, y, augment_factor=2):\n",
        "    augmented_X, augmented_y = [], []\n",
        "    for _ in range(augment_factor):\n",
        "        noise = np.random.normal(0, 0.01, X.shape)\n",
        "        scale = np.random.uniform(0.9, 1.1, X.shape)\n",
        "        X_augmented = X + noise\n",
        "        X_augmented *= scale\n",
        "        augmented_X.append(X_augmented)\n",
        "        augmented_y.append(y)\n",
        "    return np.vstack(augmented_X), np.hstack(augmented_y)\n",
        "\n",
        "# Build the optimized model\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate():\n",
        "    print(\"Loading training features...\")\n",
        "\n",
        "    # Load features\n",
        "    fake_features = load_features(FAKE_TRAIN_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_TRAIN_FEATURES_PATH)\n",
        "\n",
        "    # Validate and extract feature vectors\n",
        "    X_fake = validate_and_extract(fake_features)\n",
        "    X_real = validate_and_extract(real_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_combined = scaler.fit_transform(X_combined)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n",
        "    )\n",
        "\n",
        "    # Augment training data\n",
        "    print(\"Augmenting training data...\")\n",
        "    X_train, y_train = augment_features(X_train, y_train, augment_factor=2)\n",
        "\n",
        "    # Debugging shapes\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "    print(\"Shape of X_val:\", X_val.shape)\n",
        "    print(\"Shape of y_train:\", y_train.shape)\n",
        "    print(\"Shape of y_val:\", y_val.shape)\n",
        "\n",
        "    # Build the model\n",
        "    input_shape = (X_train.shape[1],)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min'),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
        "        ModelCheckpoint(CHECKPOINT_PATH, save_best_only=True, monitor='val_loss', mode='min')\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "        class_weight={0: 1.0, 1: 3.0}  # Adjust class weights as needed\n",
        "    )\n",
        "\n",
        "    print(\"Model training complete!\")\n",
        "\n",
        "# Run training and evaluation\n",
        "train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXWIsNyE35Zy",
        "outputId": "88168e29-a6e5-473f-9e70-d98b9d8a83d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory growth enabled.\n",
            "Loading training features...\n",
            "Augmenting training data...\n",
            "Shape of X_train: (202886, 1280)\n",
            "Shape of X_val: (25361, 1280)\n",
            "Shape of y_train: (202886,)\n",
            "Shape of y_val: (25361,)\n",
            "Starting training...\n",
            "Epoch 1/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.8468 - loss: 4.1132 - val_accuracy: 1.0000 - val_loss: 1.2801 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.8681 - val_accuracy: 0.9999 - val_loss: 0.1502 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.1291 - val_accuracy: 0.9999 - val_loss: 0.0629 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0537 - val_accuracy: 0.9999 - val_loss: 0.0373 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0373 - val_accuracy: 1.0000 - val_loss: 0.0386 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0376 - val_accuracy: 0.9999 - val_loss: 0.0270 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0315 - val_accuracy: 0.9998 - val_loss: 0.0270 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9990 - loss: 0.0414 - val_accuracy: 1.0000 - val_loss: 0.0221 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9997 - loss: 0.0241 - val_accuracy: 1.0000 - val_loss: 0.0225 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9995 - loss: 0.0278 - val_accuracy: 0.9998 - val_loss: 0.0252 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m3153/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9994 - loss: 0.0277\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9994 - loss: 0.0277 - val_accuracy: 1.0000 - val_loss: 0.0231 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0209 - val_accuracy: 1.0000 - val_loss: 0.0123 - learning_rate: 5.0000e-05\n",
            "Epoch 13/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0119 - val_accuracy: 1.0000 - val_loss: 0.0130 - learning_rate: 5.0000e-05\n",
            "Epoch 14/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0120 - val_accuracy: 1.0000 - val_loss: 0.0070 - learning_rate: 5.0000e-05\n",
            "Epoch 15/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9996 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 0.0077 - learning_rate: 5.0000e-05\n",
            "Epoch 16/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 0.0095 - learning_rate: 5.0000e-05\n",
            "Epoch 17/20\n",
            "\u001b[1m3165/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0092\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0092 - val_accuracy: 1.0000 - val_loss: 0.0098 - learning_rate: 5.0000e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0102 - val_accuracy: 1.0000 - val_loss: 0.0071 - learning_rate: 2.5000e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0095 - val_accuracy: 1.0000 - val_loss: 0.0052 - learning_rate: 2.5000e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m3171/3171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0049 - val_accuracy: 1.0000 - val_loss: 0.0045 - learning_rate: 2.5000e-05\n",
            "Model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate the model\n",
        "def validate_model():\n",
        "    print(\"Loading validation features...\")\n",
        "\n",
        "    # Load features\n",
        "    fake_valid_features = load_features(FAKE_VALID_FEATURES_PATH)\n",
        "    real_valid_features = load_features(REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "    # Validate and extract feature vectors\n",
        "    X_fake_val = validate_and_extract(fake_valid_features)\n",
        "    X_real_val = validate_and_extract(real_valid_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake_val = np.ones(len(X_fake_val))\n",
        "    y_real_val = np.zeros(len(X_real_val))\n",
        "\n",
        "    # Combine validation data and labels\n",
        "    X_val_combined = np.vstack((X_fake_val, X_real_val))\n",
        "    y_val_combined = np.hstack((y_fake_val, y_real_val))\n",
        "\n",
        "    # Normalize validation features using the same scaler from training\n",
        "    scaler = StandardScaler()\n",
        "    X_val_combined = scaler.fit_transform(X_val_combined)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = load_model(CHECKPOINT_PATH)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Predict probabilities\n",
        "    y_pred_probs = model.predict(X_val_combined)\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Metrics\n",
        "    accuracy = np.mean(y_pred == y_val_combined)\n",
        "    auc_roc = roc_auc_score(y_val_combined, y_pred_probs)\n",
        "    classification_report_text = classification_report(y_val_combined, y_pred)\n",
        "\n",
        "    # Output metrics\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report_text)\n",
        "\n",
        "# Run validation\n",
        "validate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHMttWHeUd8t",
        "outputId": "43a5c5e3-be69-4b9e-ab10-63095e039a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading validation features...\n",
            "Model loaded successfully!\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "Accuracy: 0.4932\n",
            "AUC-ROC: 0.4905\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00      1548\n",
            "         1.0       0.49      0.99      0.66      1524\n",
            "\n",
            "    accuracy                           0.49      3072\n",
            "   macro avg       0.25      0.50      0.33      3072\n",
            "weighted avg       0.25      0.49      0.33      3072\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from tensorflow.keras.mixed_precision import Policy, set_global_policy\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Enable mixed precision\n",
        "set_global_policy(Policy('mixed_float16'))\n",
        "\n",
        "# GPU Configuration: Set memory growth and limit\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    try:\n",
        "        for device in physical_devices:\n",
        "            tf.config.experimental.set_memory_growth(device, True)\n",
        "        print(\"GPU memory growth enabled.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring GPU: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected, running on CPU.\")\n",
        "\n",
        "# File paths\n",
        "FAKE_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/merged_facial_fake.pkl'\n",
        "REAL_TRAIN_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/merged_landmarks_real.pkl'\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "CHECKPOINT_PATH = \"drive/MyDrive/SP_cup/checkpoints/model_optimized.keras\"\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "\n",
        "# Function to load features\n",
        "def load_features(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Parallel feature validation and extraction\n",
        "def validate_and_extract(features):\n",
        "    def extract_feature(entry):\n",
        "        if isinstance(entry, list):\n",
        "            return [sub_entry['features'] for sub_entry in entry if isinstance(sub_entry, dict) and 'features' in sub_entry]\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            return [entry['features']]\n",
        "        return []\n",
        "\n",
        "    with Parallel(n_jobs=-1, backend='threading') as parallel:\n",
        "        results = parallel(delayed(extract_feature)(entry) for entry in features)\n",
        "    valid_features = [item for sublist in results for item in sublist]\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Augment features for robustness\n",
        "def augment_features(X, y, augment_factor=2):\n",
        "    augmented_X, augmented_y = [], []\n",
        "    for _ in range(augment_factor):\n",
        "        noise = np.random.normal(0, 0.01, X.shape)\n",
        "        scale = np.random.uniform(0.9, 1.1, X.shape)\n",
        "        X_augmented = X + noise\n",
        "        X_augmented *= scale\n",
        "        augmented_X.append(X_augmented)\n",
        "        augmented_y.append(y)\n",
        "    return np.vstack(augmented_X), np.hstack(augmented_y)\n",
        "\n",
        "# Build the optimized model\n",
        "def build_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.4),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Train and evaluate the model\n",
        "def train_and_evaluate():\n",
        "    print(\"Loading training features...\")\n",
        "\n",
        "    # Load features\n",
        "    fake_features = load_features(FAKE_TRAIN_FEATURES_PATH)\n",
        "    real_features = load_features(REAL_TRAIN_FEATURES_PATH)\n",
        "\n",
        "    # Validate and extract feature vectors\n",
        "    X_fake = validate_and_extract(fake_features)\n",
        "    X_real = validate_and_extract(real_features)\n",
        "\n",
        "    # Create labels\n",
        "    y_fake = np.ones(len(X_fake))\n",
        "    y_real = np.zeros(len(X_real))\n",
        "\n",
        "    # Combine data and labels\n",
        "    X_combined = np.vstack((X_fake, X_real))\n",
        "    y_combined = np.hstack((y_fake, y_real))\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_combined = scaler.fit_transform(X_combined)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_combined, y_combined, test_size=0.2, random_state=42, stratify=y_combined\n",
        "    )\n",
        "\n",
        "    # Augment training data\n",
        "    print(\"Augmenting training data...\")\n",
        "    X_train, y_train = augment_features(X_train, y_train, augment_factor=2)\n",
        "\n",
        "    # Debugging shapes\n",
        "    print(\"Shape of X_train:\", X_train.shape)\n",
        "    print(\"Shape of X_val:\", X_val.shape)\n",
        "    print(\"Shape of y_train:\", y_train.shape)\n",
        "    print(\"Shape of y_val:\", y_val.shape)\n",
        "\n",
        "    # Build the model\n",
        "    input_shape = (X_train.shape[1],)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min'),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n",
        "        ModelCheckpoint(CHECKPOINT_PATH, save_best_only=True, monitor='val_loss', mode='min')\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "        class_weight={0: 1.0, 1: 3.0}  # Adjust class weights as needed\n",
        "    )\n",
        "\n",
        "    print(\"Model training complete!\")\n",
        "\n",
        "    # After training, load the best model\n",
        "    model = load_model(CHECKPOINT_PATH)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"Evaluating the model...\")\n",
        "    val_predictions = model.predict(X_val, batch_size=64, verbose=1)\n",
        "    val_predictions = (val_predictions > 0.5).astype(int)  # Threshold at 0.5 for binary classification\n",
        "\n",
        "    accuracy = np.mean(val_predictions == y_val)\n",
        "    auc = roc_auc_score(y_val, val_predictions)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, val_predictions))\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n",
        "\n",
        "# Run training and evaluation\n",
        "train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "jsJfPbg24IFa",
        "outputId": "9933b2f8-aa38-4b04-879d-05353d5c624a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory growth enabled.\n",
            "Loading training features...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Found array with 0 feature(s) (shape=(2, 0)) while a minimum of 1 is required by StandardScaler.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ac50bc0b72e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;31m# Run training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-ac50bc0b72e8>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Normalize features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mX_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_combined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Split into training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \"\"\"\n\u001b[1;32m    929\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0;34m\"Found array with %d feature(s) (shape=%s) while\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(2, 0)) while a minimum of 1 is required by StandardScaler."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Load validation features\n",
        "    fake_valid_features = load_features(FAKE_VALID_FEATURES_PATH)\n",
        "    real_valid_features = load_features(REAL_VALID_FEATURES_PATH)\n",
        "\n",
        "    # Validate and extract feature vectors for validation\n",
        "    X_fake_valid = validate_and_extract(fake_valid_features)\n",
        "    X_real_valid = validate_and_extract(real_valid_features)\n",
        "\n",
        "    # Create validation labels\n",
        "    y_fake_valid = np.ones(len(X_fake_valid))\n",
        "    y_real_valid = np.zeros(len(X_real_valid))\n",
        "\n",
        "    # Combine validation data and labels\n",
        "    X_combined_valid = np.vstack((X_fake_valid, X_real_valid))\n",
        "    y_combined_valid = np.hstack((y_fake_valid, y_real_valid))\n",
        "    # After training, load the best model\n",
        "    model = load_model(CHECKPOINT_PATH)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"Evaluating the model...\")\n",
        "    val_predictions = model.predict(X_combined_valid, batch_size=64, verbose=1)\n",
        "    val_predictions = (val_predictions > 0.5).astype(int)  # Threshold at 0.5 for binary classification\n",
        "\n",
        "    accuracy = np.mean(val_predictions == y_combined_valid)\n",
        "    auc = roc_auc_score(y_combined_valid, val_predictions)\n",
        "\n",
        "    # Classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_combined_valid, val_predictions))\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"AUC-ROC: {auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHfOrbe7Z88i",
        "outputId": "c58c56f3-0402-4fbf-cfea-4b475e8fb8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model...\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.50      1.00      0.67      1548\n",
            "         1.0       0.00      0.00      0.00      1524\n",
            "\n",
            "    accuracy                           0.50      3072\n",
            "   macro avg       0.25      0.50      0.34      3072\n",
            "weighted avg       0.25      0.50      0.34      3072\n",
            "\n",
            "Accuracy: 0.5039\n",
            "AUC-ROC: 0.5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define paths\n",
        "MODEL_SAVE_PATH = \"drive/MyDrive/SP_cup/main1_sp.keras\"\n",
        "FEATURES_PATH = \"drive/MyDrive/SP_cup/features1_sp.npy\"\n",
        "LABELS_PATH = \"drive/MyDrive/SP_cup/labels1_sp.npy\"\n",
        "FAKE_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_fake_train.pkl'\n",
        "REAL_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/standardized_real_train.pkl'\n",
        "FAKE_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_fake.pkl'\n",
        "REAL_VALID_FEATURES_PATH = 'drive/MyDrive/SP_cup/features/spatial_valid_real.pkl'\n",
        "\n",
        "# Load features and labels\n",
        "def load_data(fake_path, real_path):\n",
        "    with open(fake_path, 'rb') as f:\n",
        "        fake_features = pickle.load(f)\n",
        "    with open(real_path, 'rb') as f:\n",
        "        real_features = pickle.load(f)\n",
        "\n",
        "    features = np.vstack((fake_features, real_features))\n",
        "    labels = np.hstack((np.zeros(len(fake_features)), np.ones(len(real_features))))\n",
        "    return features, labels\n",
        "\n",
        "# Load training and validation data\n",
        "print(\"Loading training data...\")\n",
        "X_train, y_train = load_data(FAKE_FEATURES_PATH, REAL_FEATURES_PATH)\n",
        "\n",
        "print(\"Loading validation data...\")\n",
        "X_valid_fake, _ = load_data(FAKE_VALID_FEATURES_PATH, FAKE_VALID_FEATURES_PATH)\n",
        "X_valid_real, _ = load_data(REAL_VALID_FEATURES_PATH, REAL_VALID_FEATURES_PATH)\n",
        "X_valid = np.vstack((X_valid_fake, X_valid_real))\n",
        "y_valid = np.hstack((np.zeros(len(X_valid_fake)), np.ones(len(X_valid_real))))\n",
        "\n",
        "# Save processed data (optional, in case needed later)\n",
        "np.save(FEATURES_PATH, X_train)\n",
        "np.save(LABELS_PATH, y_train)\n",
        "\n",
        "# Define model\n",
        "def build_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_dim=input_dim),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build and compile model\n",
        "input_dim = X_train.shape[1]\n",
        "model = build_model(input_dim)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "print(\"Training model...\")\n",
        "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate model on validation set\n",
        "print(\"Evaluating model...\")\n",
        "val_predictions = (model.predict(X_valid) > 0.5).astype(int)\n",
        "accuracy = accuracy_score(y_valid, val_predictions)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "print(\"Saving model...\")\n",
        "model.save(MODEL_SAVE_PATH)\n",
        "print(f\"Model saved at {MODEL_SAVE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "m7Im9C0R4MRH",
        "outputId": "654a931c-71db-497f-d50c-1b9df79912c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-12024a4ac417>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Load training and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFAKE_FEATURES_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREAL_FEATURES_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading validation data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-12024a4ac417>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(fake_path, real_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mreal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \"\"\"\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (16,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate and extract features\n",
        "def validate_and_extract(features):\n",
        "    valid_features = []\n",
        "    for entry in tqdm(features, desc=\"Validating and extracting features\"):\n",
        "        if isinstance(entry, list):\n",
        "            for sub_entry in entry:\n",
        "                if isinstance(sub_entry, dict) and 'features' in sub_entry:\n",
        "                    valid_features.append(sub_entry['features'])\n",
        "        elif isinstance(entry, dict) and 'features' in entry:\n",
        "            valid_features.append(entry['features'])\n",
        "    return np.array(valid_features, dtype=np.float32)\n",
        "\n",
        "# Assuming 'raw_features' is loaded from a file\n",
        "processed_features = validate_and_extract('drive/MyDrive/SP_cup/features/standardized_fake_train.pkl')\n",
        "\n",
        "# Inspect the processed features\n",
        "print(f\"Shape of processed features: {processed_features.shape}\")\n",
        "print(f\"Example feature vector: {processed_features[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "4Pq1xBelPhET",
        "outputId": "bb3e65af-7880-41d5-a02b-d222ffd5d002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating and extracting features: 100%|██████████| 57/57 [00:00<00:00, 417963.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of processed features: (0,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0f040bbbfc0e>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Inspect the processed features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of processed features: {processed_features.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Example feature vector: {processed_features[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WfP35t_7Ppqt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}